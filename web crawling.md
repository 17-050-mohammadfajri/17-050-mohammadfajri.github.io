Apa itu *web crawling*?

Dikutip dari [Totally Tech](https://totally.tech/quick-guides/search-engine-basics-crawling-indexing-ranking/), *web crawling *adalah proses di mana *search engine *menemukan konten yang di-*update* di sebuah situs atau halaman baru, perubahan situs, atau *link *yang mati.

Adapun menurut [Moz](https://moz.com/beginners-guide-to-seo/how-search-engines-operate), *web crawling *adalah proses di mana mesin pencari mengirimkan tim robot (*crawler *atau *spider*) untuk menemukan konten-konten baru dan konten yang telah di-*update.*

Konten yang dimaksud bisa bervariasi, mulai dari halaman *website, *gambar, video, dokumen, dan lain sebagainya.

Jika masih belum paham, kamu bisa membayangkan seekor laba-laba. 

Laba-laba datang ke sebuah jaring dan melihat beberapa halaman *website, *kemudian mengikuti *link *yang terdapat di halaman *website *tersebut untuk mencari URL yang baru.

Dengan mendatangi berbagai URL tersebut, laba-laba akan menemukan konten baru dan memasukkannya dalam indeks mereka.

Indeks di sini berarti sebuah *database *berisi URL yang telah ditemukan oleh laba-laba tersebut.

Ketika ada pengguna yang mencari sebuah konten di *search engine *dengan [*keyword*](https://glints.com/id/lowongan/jenis-keyword/#.XykVuhMzZQI) tertentu, *search engine *akan mencarinya di indeks dan menentukan konten mana yang paling sesuai untuk pengguna tersebut.

Proses *web crawling *tidak dapat dilakukan secara manual. Ada beragam pilihan *tools *yang harus digunakan.

*Tools *untuk *web crawling *tersebut adalah *web crawler *yang sering juga disebut sebagai *web robot *atau *web spider.*

Sebuah *website *tidak dapat ditemukan begitu saja oleh *search engine.*

*Search engine *harus melakukan *crawling *dan *indexing *sebelum akhirnya menampilkan konten *website *pada SERP mereka.

Proses ini dilakukan dengan bantuan *tools *yang disebut *web crawler, web robot, *atau *web spider.*

Pada dasarnya, *web crawler *melakukan tugas sesuai namanya, yaitu *crawling. Web crawler *akan menemukan konten di berbagai *website *untuk diindeks dalam *search engine.*

Lalu, bagaimana proses kerja sebuah *web crawler*?

*Pertama, web crawler *akan mengunjungi sebuah situs dan berbagai *link *yang terdapat dalam laman tersebut. Namun jika situsmu terbilang baru dan belum ada *link *lain di dalamnya, kamu bisa meminta *search engine *untuk mendatangi situsmu, seperti dikutip dari [WebFX](https://www.webfx.com/blog/internet/what-is-a-web-crawler/#web-crawler-examples).

Caranya mudah. Kamu hanya perlu memasukkan URL situsmu di Google Search Console.

Kemudian, tugas *tools web crawling* berikutnya adalah mencatat setiap *link *yang mereka temukan ke indeks mereka.

Namun, perlu kamu catat bahwa *web crawler *hanya akan mengumpulkan informasi dari laman yang bersifat publik, ya. *Web crawler *tidak ikut mencatat laman *private* yang tidak dapat diakses.

Setelah itu, *web crawler *akan mengumpulkan berbagai informasi, seperti tulisan dan *meta tag.*

Informasi tersebut akan tersimpan dalam indeks *search engine *sehingga dapat muncul ketika pengguna mencari konten dengan *keyword *yang serupa.

Ada beragam pilihan *web crawler *yang bisa kamu gunakan. Beberapa di antaranya gratis, tetapi ada juga yang berbayar.

Beberapa contoh *tools *populer untuk *web crawling *adalah sebagai berikut.

### 1. Googlebot

Googlebot adalah *web crawler *yang paling banyak digunakan saat ini. Seperti namanya, *web crawler *ini adalah milik Google.

Googlebot mengumpulkan berbagai dokumen yang ada di sebuah [*website*](https://glints.com/id/lowongan/jenis-website/#.XykV2hMzZQI) untuk membuat indeks yang dapat dicari oleh *search engine *Google.

*Web crawler *yang satu ini merujuk pada dua jenis *web crawler, *yaitu *desktop crawler *dan *mobile crawler.*

### 2. HTTrack

HTTrack adalah *web crawler* yang bersifat* open source*. Kamu bisa men-*download *situs *world wide web *(www) dari internet ke komputermu sehingga kamu bisa melihatnya secara *offline.*

Jika sudah men-*download *konten situs tersebut, kamu bisa membukanya melalui *browser*-mu tanpa koneksi internet.

### 3. Cyotek Webcopy

Serupa dengan HTTrack, Cyotek Webcopy dapat digunakan untuk men-*download *situs dari internet ke komputermu.

Salah satu kelebihan *web crawler *ini adalah memungkinkan penggunanya memilih bagian yang ingin di-*download. *Jadi, kamu bisa memilih apakah ingin men-*download *semua bagian situs, foto tertentu, dan sebagainya.

### 4. Webhose

Contoh *web crawler *berikutnya adalah Webhose.

Webhose adalah *web crawler *yang dapat mengubah konten *website *yang tidak terstruktur menjadi *data feeds *yang dapat dibaca oleh mesin.

*Data feeds *yang dimaksud dapat mencakup banyak sumber data, seperti diskusi *online, *situs berita, dan lainnya.

#### Baca Juga: [SEO Tools yang Wajib Dikuasai agar Websitemu Optimal](https://glints.com/id/lowongan/seo-tools-yang-wajib-dikuasai-agar-websitemu-optimal/#.XykU0hMzZQI)

Demikian penjelasan Glints tentang apa itu *web crawling *dan *web crawler.*

Kesimpulannya, *web crawling *adalah proses mencari tahu kumpulan halaman dari sebuah web untuk dilakukan pengindeksan.

*Web crawling *sangat berguna bagi sebuah *website *agar mudah ditemukan oleh orang lain. Namun, *web crawling *bukan satu-satunya cara yang bisa dilakukan.